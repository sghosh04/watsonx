{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# This notebook illustrates how to deploy a tuned model in watsonx.ai July 7, 2023 GA release\n### #\n#### Author: Shuvanker Ghosh - sghosh@us.ibm.com\n####\n#### In this example notebook, I have a tuned model endpoint, which I then deployed on watsonx.ai using python function wrapper approach for consumption of the model for generating use case specific content"}, {"metadata": {}, "cell_type": "markdown", "source": "### 1. Define the deployable function\nDefine a Python closure with an inner function named \"score\" which then connects to tuned model endpoint to get the generated text for a given input"}, {"metadata": {"id": "e4beb67d-375e-46f8-acce-f5102de658b1"}, "cell_type": "code", "source": "#wml_python_function\ndef my_deployable_function():\n    import requests\n    class FoundationModelWrapper:\n        def __init__(self, model_end_point, apikey, model_id, model_parameters):\n            self.model_end_point = model_end_point\n            self.apikey = apikey\n            self.model_id = model_id\n            self.model_parameters = model_parameters\n\n        def generate(self, model_input):\n            \n            # Access token may be needed for some cases\n            # from ibm_cloud_sdk_core import IAMTokenManager\n            # from ibm_cloud_sdk_core.authenticators import IAMAuthenticator, BearerTokenAuthenticator\n            # access_token = IAMTokenManager(\n            # apikey =  self.apikey,\n            # url = \"https://iam.cloud.ibm.com/identity/token\"\n            #).get_token()\n\n            headers = {\n                \"Authorization\": \"Bearer \" + self.apikey,\n                \"Content-Type\": \"application/json\",\n                \"Accept\": \"application/json\"\n            }\n            data = {\n                \"model_id\": self.model_id,\n                \"inputs\": [model_input],\n                \"parameters\": self.model_parameters\n            }\n            response = requests.post(self.model_end_point, json=data, headers=headers)\n            if response.status_code == 200:\n                return response.json()[\"results\"][0][\"generated_text\"]\n            else:\n                return response.text\n\n    def score( payload ):\n        import os\n\n        apikey = \"PASTE YOUR API KEY\"\n\n        # Set tuned model end point\n        model_end_point = \"PASTE YOU TUNED MODEL ENDPOINT\"\n        model_end_point = \"https://workbench-api.res.ibm.com/v1/generate\"\n    \n        \n        # Set tuned model id\n        model_id = \"PASTE YOUR TUNED FOUNDATION MODEL ID\"\n        model_id = \"flan-t5-xl-mpt-tcFvBXTp-2023-07-11-17-05-03\"\n        #model_id = \"google/flan-ul2\"\n        \n        # Set model parameters\n        model_parameters = {\n            \"decoding_method\": \"greedy\",\n            \"min_new_tokens\": 1,\n            \"max_new_tokens\": 200,\n            \"beam_width\": 1\n        }\n        \n        # Create the wrapper\n        wrapper = FoundationModelWrapper(model_end_point, apikey, model_id, model_parameters)\n        \n        # Get the mode input from the payload\n        model_input = payload.get(\"input_data\")[0].get(\"values\")[0][0]\n\n        # Generate and capture model output\n        model_output = wrapper.generate(model_input)\n        \n        # Score using the pre-defined model\n        score_response = {\n            'predictions': [{'fields': ['Response_message_field'], \n                             'values': [[model_output]]\n                            }]\n        } \n        return score_response\n       \n\n    return score", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2. Test locally\nTest your function in the notebook before deploying the function."}, {"metadata": {}, "cell_type": "code", "source": "# Pass the sample input to the function as a test\n\nfunc_result = my_deployable_function()({\"input_data\": [{\"values\": [[\"My water cooler is not dispensing\"]]}]})\nprint(func_result)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3. Set up deployment enviornment\nSet up the WML environment to deploy the foundation model wrapper function\nPre requisite: Create a deployment space and note down the space_id"}, {"metadata": {}, "cell_type": "code", "source": "# Setup API Key and WML credentials\napikey = 'PASTE YOUR IBM CLOUD API KEY'\nlocation = 'us-south'\n\nwml_credentials = {\n    \"apikey\": apikey,\n    \"url\": 'https://' + location + '.ml.cloud.ibm.com'\n}\n\nfrom ibm_watson_machine_learning import APIClient\n\n# WML client\nclient = APIClient(wml_credentials)\n\n# Set space_id\nspace_id = 'PASTE YOUR SPACE ID HERE'\n\nclient.spaces.list(limit=20)\n\n# Set default space\nclient.set.default_space(space_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 4. Store and deploy the function\nBefore you can deploy the function, you must store the function in your Watson Machine Learning repository."}, {"metadata": {}, "cell_type": "code", "source": "# Look up software specification for the deployable function\nsofware_spec_uid = client.software_specifications.get_id_by_name(\"runtime-22.2-py3.10\")\n\nmodel_name = \"Service Request Categorization\"\n\n\n# Store the deployable function in your Watson Machine Learning repository\nmeta_data = {\n    client.repository.FunctionMetaNames.NAME: f\"{model_name} Function\",\n    client.repository.FunctionMetaNames.SOFTWARE_SPEC_UID: sofware_spec_uid\n}\n\nfunction_details = client.repository.store_function(meta_props=meta_data, function=my_deployable_function)\n\n# Get published function ID\nfunction_uid = client.repository.get_function_uid(function_details)\nprint (function_uid)\n\n# Deploy the stored function\nmetadata = {\n    client.deployments.ConfigurationMetaNames.NAME: f\"{model_name} Function Deployment\",\n    client.deployments.ConfigurationMetaNames.ONLINE: {}\n}\n\nfunction_deployment_details = client.deployments.create(function_uid, meta_props=metadata)\nprint(function_deployment_details)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 5. Test the deployed function\nUse the Watson Machine Learning Python client to send data to your function deployment for processing in exactly the same way you send data to model deployments for processing."}, {"metadata": {}, "cell_type": "code", "source": "# Get the endpoint URL of the function deployment just created\nfunction_deployment_id = client.deployments.get_uid(function_deployment_details)\nfunction_deployment_endpoint_url = client.deployments.get_scoring_href(function_deployment_details)\nprint(function_deployment_id)\nprint(function_deployment_endpoint_url)\n\n# Test payload\npayload = {\"input_data\": [{\"values\": [[\"Dryer is not heating up\"]]}]}\n\n# Score\nresult = client.deployments.score(function_deployment_id, payload)\nif \"error\" in result:\n    print(result[\"error\"])\nelse:\n    print(result)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}